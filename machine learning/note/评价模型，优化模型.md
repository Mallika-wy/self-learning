## Evaluating model

### 测试集和交叉验证集

Taking a dataset and splitting it into a training set and a separate test set gives you a way to systematically evaluate how well our learning outcomes doing. By computing both $J_{test} \quad and \quad J_{train}$, we can measure how was doing on the test set and on the training set.

对于不同的模型， $J_{test}$和$J_{cv}$都是不同的：

**对于linear regression：**

computer test error：
$$
J_{test}(\vec w,b)=\frac{1}{2m_{test}}[\sum_{i=1}^{m_{test}}(f_{\vec w,b}(\vec x_{test}^{(i)})-y_{test}^{(i)})^2]
$$
computer validation error：
$$
J_{cv}(\vec w,b)=\frac{1}{2m_{cv}}[\sum_{i=1}^{m_{cv}}(f_{\vec w,b}(\vec x_{cv}^{(i)})-y_{cv}^{(i)})^2]
$$
compute training error:
$$
J_{train}(\vec w,b)=\frac{1}{2m_{train}}[\sum_{i=1}^{m_{train}}(f_{\vec w,b}(\vec x_{train}^{(i)})-y_{train}^{(i)})^2]
$$
**对于classification problem：**

computer test error：
$$
J_{test}(\vec w,b)=-\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}[y_{test}^{(i)}log(f_{\vec w,b}(\vec x_{test}^{(i)})+(1-y_{test}^{(i)})log(1-f_{\vec w,b}(\vec x_{test}^{(i)}))]
$$
compute validation error:
$$
J_{cv}(\vec w,b)=-\frac{1}{m_{cv}}\sum_{i=1}^{m_{cv}}[y_{cv}^{(i)}log(f_{\vec w,b}(\vec x_{cv}^{(i)})+(1-y_{cv}^{(i)})log(1-f_{\vec w,b}(\vec x_{cv}^{(i)}))]
$$
compute train error:
$$
J_{train}(\vec w,b)=-\frac{1}{m_{train}}\sum_{i=1}^{m_{train}}[y_{train}^{(i)}log(f_{\vec w,b}(\vec x_{train}^{(i)})+(1-y_{train}^{(i)})log(1-f_{\vec w,b}(\vec x_{train}^{(i)}))]
$$


### 